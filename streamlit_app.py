

import subprocess
import sys
import pkg_resources
import kagglehub.models  


def install_requirements():
    try:
        with open("requirements.txt", "r") as file:
            required_packages = [line.strip() for line in file.readlines() if line.strip()]

        installed_packages = {pkg.key for pkg in pkg_resources.working_set}
        missing_packages = [pkg for pkg in required_packages if pkg.split("==")[0] not in installed_packages]

        if missing_packages:
            print(f"üì¶ Installation des packages manquants : {missing_packages}")
            subprocess.check_call([sys.executable, "-m", "pip", "install", *missing_packages])
            print("‚úÖ Toutes les d√©pendances sont maintenant install√©es.")
        else:
            print("‚úÖ Toutes les d√©pendances sont d√©j√† install√©es.")

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la v√©rification/installation des d√©pendances : {e}")

# Ex√©cuter la fonction
install_requirements()

import subprocess
import sys
import pkg_resources  
import streamlit as st
import pandas as pd
import math
from pathlib import Path
import kagglehub
import matplotlib.pyplot as plt
import seaborn as sns
import unidecode

# --- Configuration de la page ---
st.set_page_config(
    page_title="Analyse de l'Esp√©rance de Vie - OMS",
    page_icon="üåç",
    layout="wide"
)

# --- import du dataset ---
path = kagglehub.dataset_download("kumarajarshi/life-expectancy-who")  # ou sp√©cifie le chemin complet si le fichier est ailleurs
dataset_file = f"{path}/Life Expectancy Data.csv"
df = pd.read_csv(dataset_file)

# --- Texte de pr√©sentation ---
left_co, cent_co, last_co = st.columns([1, 3, 1])
imgleft_co, imgcent_co, imglast_co = st.columns(3)
with cent_co:
    st.title(" Analyse de l'Esp√©rance de Vie - OMS")
with imgcent_co:
    st.image("images/oms_logo.png", width=200)

st.write("")
st.markdown("""---""")

# --- chapitre sur pr√©senttation du dataset ---
left_co, cent_co, last_co = st.columns([1, 3, 1])
with cent_co:
    st.markdown("""
                 # **Pr√©diction de l'esp√©rance de vie**
                """)
    st.markdown("""
    Ce projet √† pour objectif de d√©finir les facteurs influen√ßant l'esp√©rance de vie √† travers diff√©rents pays et ann√©es.
                """)
left_co, cent_co, last_co = st.columns([1, 3, 1])
with cent_co:
    st.markdown("""
                 ## **Hypoth√©se:**
                 Peut on d√©finir l'√©sp√©rence de vie d'une personne en fonction d'un ensemble d'√©l√©ments donn√©s?
                """)
    
st.markdown("""
            
üîé **Objectifs** :
- Explorer les facteurs cl√©s qui influencent la long√©vit√©.
- Nettoyer et pr√©parer les donn√©es.
- Comparer diff√©rents mod√®les de pr√©diction.
- Pr√©senter les r√©sultats de mani√®re interactive
            
üìå **M√©thodologie** :
1. **Exploration des donn√©es** 
2. **Nettoyage des valeurs manquantes** 
3. **Mod√©lisation avec Machine Learning** 
4. **Interpr√©tation des r√©sultats** 
---
""", unsafe_allow_html=True)



# Aper√ßu du dataset
st.subheader("Aper√ßu du dataset BRUT")
st.write(df.head())


# Pr√©sentation des colonnes du dataset
st.subheader("Pr√©sentation des colonnes du dataset")

left_co, center_co, right_co = st.columns([1,2,1])
with center_co:
    st.markdown("""
                ### ‚ö†Ô∏è note: 
                **Life expectancy** sera notre variable **" Y "**. 
                il s'agit de l'√©l√©ment que l'on souhaite pr√©dire.
                """)

left_co, center_co, right_co = st.columns(3)
with left_co:
    st.markdown("""
        1. **Life expectancy** : L'esp√©rance de vie moyenne (en ann√©es) pour le pays et l'ann√©e donn√©s.
        2. **Country** : Le pays correspondant aux donn√©es.
        3. **Year** : L'ann√©e o√π les donn√©es ont √©t√© collect√©es.
        4. **Adult Mortality** : Taux de mortalit√© des adultes (probabilit√© de d√©c√®s entre 15 et 60 ans, par 1000 habitants).
        5. **Infant deaths** : Nombre de d√©c√®s d'enfants de moins de 1 an pour 1000 naissances vivantes.
        6. **Alcohol** : Consommation moyenne d'alcool (litres par habitant par an).
        7. **Percentage expenditure** : D√©penses publiques pour la sant√© (% du PIB).
        """, unsafe_allow_html=True)
    with center_co:
        st.markdown("""
            8. **Hepatitis B** : Taux de couverture vaccinale contre l'h√©patite B chez les enfants (%).
            9. **Measles** : Nombre de cas signal√©s de rougeole.
            10. **BMI** : Indice de masse corporelle moyen (IMC) pour la population.
            11. **Under-five deaths** : Nombre de d√©c√®s d'enfants de moins de 5 ans pour 1000 naissances vivantes.
            12. **Polio** : Taux de couverture vaccinale contre la poliomy√©lite chez les enfants (%).
            13. **Total expenditure** : D√©penses totales pour la sant√© (% du PIB).
            14. **Diphtheria** : Taux de couverture vaccinale contre la dipht√©rie chez les enfants (%).
            """, unsafe_allow_html=True)
    with right_co:
        st.markdown("""
            15. **HIV/AIDS** : D√©c√®s dus au VIH/sida pour 1000 habitants.
            16. **GDP** : Produit int√©rieur brut par habitant (USD).
            17. **Population** : Population totale du pays.
            18. **Thinness 1-19 years** : Pr√©valence de la maigreur chez les enfants et adolescents (1 √† 19 ans) (%).
            19. **Thinness 5-9 years** : Pr√©valence de la maigreur chez les enfants (5 √† 9 ans) (%).
            20. **Income composition of resources** : Indicateur composite de revenus (entre 0 et 1).
            21. **Schooling** : Dur√©e moyenne de scolarisation (ann√©es).
            """, unsafe_allow_html=True)

#  --- statistique desctiptives: 
st.markdown("""
            ---
            """)

left_co, center_co, right_co = st.columns(3)
# Analyse descriptive des donn√©es
with center_co:
    st.subheader("Analyse descriptive du dataset")

# Aper√ßu g√©n√©ral
st.markdown("""
### **üîç Analyse descriptive**
L'objectif de cette section est d'explorer les statistiques de base du dataset et d'identifier les premi√®res observations cl√©s.

Voici ce que nous pouvons examiner :
""")
chart_type = st.radio("Choisissez le type de graphique :", 
                      ["1. Taille du dataset", "2. Types de donn√©es par colonne", "3. Pr√©sence de valeurs manquantes", "4. Statistiques descriptives des variables num√©riques"])

# Taille du dataset
if chart_type == "1. Taille du dataset":
    st.markdown("## **Taille du dataset**")
    st.write(f"**Nombre de lignes** : {df.shape[0]}")
    st.write(f"**Nombre de colonnes** : {df.shape[1]}")

# Types de donn√©es par colonne
if chart_type == "2. Types de donn√©es par colonne":
    left_co, right_co = st.columns(2)
    with left_co:
        st.markdown("""
            ## **Types de donn√©es par colonne**
            """)
        st.write(df.dtypes)
    with right_co:
        st.markdown("""
                    ## **Observations**
                    - **titre des colone** nous devons formatter les titres des collones.
                    - les variables de type "object" (cat√©goriques) doivent √™tre converties en valeurs num√©riques pour que les mod√®les puissent les utiliser correctement.
                    - **nettoyage et normalisation des donn√©es** Nous devons formatez les lignes dans les colonnes pour √™tres sur qu'il n'y ai pas d'erreur de saisie (Espace, Majuscule...) 
                    - ‚ö†Ô∏è **country**: Initialement, nous souhaitions utiliser la m√©thode **Encodage One-Hot Encoding**.
                    comme nous allons le voir juste apr√©s, cette m√©thode n'est pas viable dans notre cas.
                    - **status** : Ici, nous utiliserons la m√©thode **Label Encoding**.
                    """)
        # --- G√©n√©ration du Heatmap des Corr√©lations ---
if chart_type == "2. Types de donn√©es par colonne":
    left_co, center_co, right_co = st.columns([1,3,1])
    with center_co:   
        st.markdown("""
                    ### S√©lection des Variables pour Identifier le Pays:
                    ‚ö†Ô∏è **Country** : Initialement, je souhaitais utiliser la m√©thode **One-Hot Encoding**, qui cr√©e une colonne pour chaque pays et assigne une valeur de 0 ou 1 en fonction de l'appartenance de la ligne √† un pays.  
                    
                    ‚ö†Ô∏è **Cependant** : cette m√©thode g√©n√®re un trop grand nombre de colonnes, ce qui entra√Æne une **mal√©diction de la dimensionnalit√©**.  

                    Je vais donc chercher **deux variables faiblement corr√©l√©es entre elles**, mais significatives pour diff√©rencier les pays (par exemple, le PIB s'il est disponible). Ensuite, nous cr√©erons une **nouvelle variable** en effectuant une op√©ration entre ces deux variables :  
                    `nouvelle_variable = variable1 / (variable2 + 1)`.  

                    Pour cela, une **heatmap** nous aidera √† identifier les deux variables les plus pertinentes.            
                    """)


        # S√©lectionner les colonnes num√©riques
        numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns.tolist()

        # V√©rifier qu'on a bien des colonnes num√©riques
        if not numeric_columns:
            st.warning("‚ö†Ô∏è Aucune colonne num√©rique disponible pour la corr√©lation.")
        else:
            # Calcul de la matrice de corr√©lation
            correlation_matrix = df[numeric_columns].corr()

            # Cr√©ation du heatmap
            fig, ax = plt.subplots(figsize=(12, 8))
            sns.heatmap(correlation_matrix, annot=False, cmap="coolwarm", linewidths=0.5, center=0)
            plt.title("Matrice de Corr√©lation des Variables")
            st.pyplot(fig)

            # S√©lectionner deux variables avec une **faible corr√©lation** entre elles
            st.markdown("### üìâ S√©lection de Variables Faiblement Corr√©l√©es")

            # Conversion de la matrice de corr√©lation en DataFrame pour l'analyse
            corr_pairs = correlation_matrix.unstack().reset_index()
            corr_pairs.columns = ["Variable 1", "Variable 2", "Corr√©lation"]

            # √âviter les doublons et les corr√©lations parfaites (1.0 avec soi-m√™me)
            corr_pairs = corr_pairs[corr_pairs["Variable 1"] != corr_pairs["Variable 2"]]
            corr_pairs["Corr√©lation"] = corr_pairs["Corr√©lation"].abs()  # Valeur absolue pour √©viter les signes
            corr_pairs = corr_pairs.sort_values("Corr√©lation")  # Trier par corr√©lation croissante

            # Affichage des **10 paires de variables les moins corr√©l√©es**
            st.write("Top 10 des paires de variables avec une faible corr√©lation (meilleur choix pour encoder le pays) :")
            st.write(corr_pairs.head(10))

            # --- G√©n√©ration de la nouvelle variable country_index ---
            # Nettoyage des noms de colonnes : suppression des espaces avant et apr√®s
            # Nettoyage des noms de colonnes : suppression des espaces
            df.columns = df.columns.str.strip()

            # V√©rification que les colonnes n√©cessaires sont bien pr√©sentes
            if "Total expenditure" in df.columns and "HIV/AIDS" in df.columns:
                # Cr√©ation de la colonne country_index
                df["country_index"] = df["Total expenditure"] / (df["HIV/AIDS"] + 1)  # √âvite la division par z√©ro

                # Mettre √† jour les donn√©es nettoy√©es dans st.session_state
                st.session_state["df_cleaned"] = df

                st.success("‚úÖ Nouvelle colonne `country_index` cr√©√©e avec succ√®s !")
                st.write("üìå **Colonnes disponibles apr√®s la cr√©ation** :", df.columns.tolist())

                # Affichage pour v√©rifier si la colonne est bien ajout√©e
                st.write(df[["Country", "Total expenditure", "HIV/AIDS", "country_index"]].head())

                df = df.drop(columns=["Country"])

        

# Pr√©sence de valeurs manquantes
if chart_type == "3. Pr√©sence de valeurs manquantes":
    st.markdown("## **Pr√©sence de valeurs manquantes**")
    missing_values = df.isnull().sum()
    missing_percent = (df.isnull().sum() / len(df)) * 100
    missing_data = pd.DataFrame({
        "Valeurs manquantes": missing_values,
        "Pourcentage (%)": missing_percent
    })
    st.write(missing_data)

if chart_type == "4. Statistiques descriptives des variables num√©riques":
    st.markdown("## **Statistiques descriptives des variables num√©riques**")
    
    # Calcul des statistiques descriptives
    descriptive_stats = df.describe().transpose()

    # Ajout d'un format plus lisible
    descriptive_stats = descriptive_stats.rename(columns={
        "count": "Nombre de valeurs",
        "mean": "Moyenne",
        "std": "√âcart-type",
        "min": "Valeur min",
        "25%": "1er quartile (Q1)",
        "50%": "M√©diane (Q2)",
        "75%": "3e quartile (Q3)",
        "max": "Valeur max"
    })

    # Affichage sous forme de tableau interactif
    st.write(descriptive_stats)

# ----------------------------------------------------
#  pr√©sentation cleaning
# ----------------------------------------------------
def clean_dataset(df):
    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')
    for col in ["country", "status"]:
        if col in df.columns:
            df[col] = df[col].astype(str).str.strip().str.lower()
            df[col] = df[col].apply(lambda x: unidecode.unidecode(x))  # Supprimer accents
    return df

def imput_null(df):
    columns_median = ["life_expectancy", "adult_mortality", "alcohol", "total_expenditure", 
                "thinness_5-9_years", "thinness__1-19_years", "bmi", ]
    for col in columns_median:
        if col in df.columns:  # V√©rifier si la colonne existe avant d'appliquer
            df[col].fillna(df[col].median(), inplace=True)
    # Moyenne pour les indicateurs de vaccination (plus de groupby, uniquement moyenne globale)
    columns_vaccination = ["hepatitis_b", "polio", "diphtheria"]
    for col in columns_vaccination:
        if col in df.columns:
            df[col].fillna(df[col].mean(), inplace=True)  
    # Moyenne pour les indicateurs √©conomiques et d'√©ducation
    columns_mean = ["income_composition_of_resources", "schooling"]
    for col in columns_mean:
        if col in df.columns:
            df[col].fillna(df[col].mean(), inplace=True)
    # Imputation bas√©e sur la m√©diane globale pour GDP et Population (pas de groupby)
    columns_gdp_population = ["gdp", "population", "bmi", "thinness__1-19_years", "thinness_5-9_years" ]
    for col in columns_gdp_population:
        if col in df.columns:
            df[col].fillna(df[col].median(), inplace=True)

df = clean_dataset(df)

# radio:
left_co, center_co, right_co = st.columns([1,3, 1])
with center_co:
    st.markdown(""" # DATA-CLEANSING """)
    chart_type = st.radio(" ", 
                      ["Typages des Donn√©es", "Label Encoding (status)", "Remplacement des valeurs manquantes"])
    if chart_type == "Typages des Donn√©es":
        st.markdown(""" 
                ### **Typages des Donn√©es** 
                Ici, nous formatons le titre des colones, ainsi que toutes les lignes afin d'√©tre sur d'avoir une nomenclature identique pour chaque colonne, et pour chaque objet.
                """)
        st.write(df.head(10).round(2))
        df = df.round(2)
    
    if chart_type == "Label Encoding (status)":
        st.markdown(""" 
                ### **Label Encoding (status)** 
                Ici, nous transformons les valeurs string des status en 0 ou 1 en fonction de la valeurs initiale du status.
                """)
        from sklearn.preprocessing import LabelEncoder

        # Initialiser le label encoder
        label_encoder = LabelEncoder()

        # Transformer la colonne Status en 0 et 1
        df["status_encoded"] = label_encoder.fit_transform(df["status"])

        # V√©rifier le mapping des valeurs
        status_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
        print("Mapping des valeurs :", status_mapping)

        # Supprimer l'ancienne colonne si n√©cessaire
        df = df.drop(columns=["status"])
        st.write(df.head())


if chart_type == "Remplacement des valeurs manquantes":
    col_right, col_left = st.columns(2)
    with col_right:
        st.markdown(""" 
                ### **Remplacement des valeurs manquantes** 
                Les valeures manquantes peuvent impacter la qualit√© de l'analyse et fausser les mod√®les pr√©dictifs.  
                Voici une analyse d√©taill√©e des colonnes concern√©es et les strat√©gies retenues pour leur traitement.
            ---

            ## üõ† M√©thodes d'imputation retenues :
            - **M√©diane** : utilis√©e pour √©viter l'effet des valeurs extr√™mes (outliers).
            - **Moyenne par pays** : pr√©f√©r√©e pour les indicateurs influenc√©s par le pays (vaccination, √©ducation).
            - **M√©diane par pays & ann√©e** : pour les variables comme la **Population** et le **PIB**, tr√®s h√©t√©rog√®nes.

            ---
                    
            ## Colonnes avec valeurs manquantes et strat√©gies de traitement
                    
            ---

            | **Colonne**                      | **% de valeurs manquantes** | **Statistiques cl√©s** | **Strat√©gie d'imputation** |
            |----------------------------------|---------------------------|-----------------------|----------------------------|
            | **Life expectancy**              | 0.34%                     | M√©diane = 72.1, √âcart-type = 9.52 | **M√©diane** : √©vite l'impact des outliers. |
            | **Adult Mortality**              | 0.34%                     | M√©diane = 144, √âcart-type = 124.29 | **M√©diane** : car forte dispersion des valeurs. |
            | **Alcohol**                      | 6.60%                     | M√©diane = 3.75, √âcart-type = 4.05 | **M√©diane** : variable √† forte variabilit√©. |
            | **Hepatitis B**                  | 18.82%                    | M√©diane non fournie | **Moyenne par pays** : taux de vaccination. |
            | **BMI**                          | 1.15%                     | M√©diane = 22.4 | **M√©diane** : homog√©n√©it√© relative des valeurs. |
            | **Polio**                        | 0.65%                     | M√©diane non fournie | **Moyenne par pays** : taux de vaccination. |
            | **Total expenditure**             | 7.69%                     | M√©diane = 5.85 | **M√©diane** : √©vite les extr√™mes. |
            | **Diphtheria**                   | 0.65%                     | M√©diane non fournie | **Moyenne par pays** : taux de vaccination. |
            | **GDP**                          | 15.25%                    | M√©diane = 3457, √âcart-type = 15,312 | **M√©diane par pays** : tr√®s forte dispersion du PIB. |
            | **Population**                   | 22.19%                    | M√©diane non fournie | **M√©diane par pays & ann√©e** : donn√©e tr√®s variable. |
            | **Thinness 1-19 years**          | 1.15%                     | M√©diane = 2.3 | **M√©diane** : variable peu dispers√©e. |
            | **Thinness 5-9 years**           | 1.15%                     | M√©diane = 2.3 | **M√©diane** : m√™me raison que Thinness 1-19 ans. |
            | **Income composition of resources** | 5.68%                  | M√©diane = 0.62 | **Moyenne par pays** : refl√®te les disparit√©s √©conomiques. |
            | **Schooling**                    | 5.55%                     | M√©diane = 12.3 | **Moyenne par pays** : car impact fort du pays. |

            ---
                            
            """)
        with col_left:
            st.write("")
            st.markdown("""
                        lignes apr√©s traitements des valeurs manquantes
                        """)
            imput_null(df)
            st.write(df.isnull().sum())

imput_null(df)


    # ---------------------------------------------
# --- Interface utilisateur ---
st.subheader("üîç Visualisation interactive des donn√©es")

# Liste des colonnes num√©riques pour l‚Äôanalyse
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
# S√©lection du type de graphique
col_left, col_cent, col_right = st.columns([2,2,2])
st.markdown("""
    ### Analyser en d√©tail les diff√©rentes variables du dataset afin de mieux comprendre leur distribution et d'identifier d'√©ventuelles anomalies.
    **S√©lection interactive d'une colonne**
    Choix d'une colonne num√©rique parmi celles du dataset pour afficher ses statistiques et visualiser sa distribution.

    **Affichage d'un histogramme et d'un boxplot**
    L'histogramme permet d'observer la fr√©quence des valeurs et leur r√©partition, tandis que le boxplot met en √©vidence la dispersion des donn√©es et d√©tecte les valeurs extr√™mes.

    **D√©tection automatique des outliers**
    Les valeurs aberrantes sont identifi√©es √† l'aide de la m√©thode de l'IQR (Interquartile Range), qui rep√®re les valeurs situ√©es en dehors de la plage normale des donn√©es.

    **Option pour exclure les outliers**
    Nous avons si besoin la possibilit√© d'exclure les valeurs aberrantes afin d'observer leur impact sur la distribution et d'analyser uniquement les donn√©es consid√©r√©es comme repr√©sentatives.
    """)   

# --- Exploration avanc√©e des donn√©es ---
st.markdown("---")

if "df_cleaned" not in st.session_state:
    st.session_state["df_cleaned"] = df.copy()  # On garde une copie propre des donn√©es

# S√©lection d'une colonne pour l'analyse
numeric_columns = st.session_state["df_cleaned"].select_dtypes(include=['float64', 'int64']).columns.tolist()
selected_column = st.selectbox("S√©lectionnez une colonne :", numeric_columns)
col_1, col_2, col_3 = st.columns([2,2,2])
with col_2:
    st.write(f"## {selected_column}")

col_1, col_2, col_3 = st.columns([1,2,2])
with col_1:
    if selected_column:
        # Afficher les statistiques descriptives
        st.write(f"### üìä Statistiques descriptives")
        st.write(st.session_state["df_cleaned"][selected_column].describe())
with col_2:
    if selected_column:
        # Histogramme avec KDE
        st.write(f"### üìà Distribution")
        fig, ax = plt.subplots(figsize=(8, 5))
        sns.histplot(st.session_state["df_cleaned"][selected_column], bins=50, kde=True, ax=ax)
        plt.xlabel(selected_column)
        plt.ylabel("Fr√©quence")
        st.pyplot(fig)

with col_3:
    if selected_column:
        # Boxplot pour identifier les outliers
        st.write(f"### üìä Boxplot")
        fig, ax = plt.subplots(figsize=(8, 5))
        sns.boxplot(x=st.session_state["df_cleaned"][selected_column], ax=ax)
        plt.xlabel(selected_column)
        st.pyplot(fig)
col_1, col_2, col_3 = st.columns([2,2,2])
with col_2:
    st.write(f"### D√©tection des outliers avec IQR: {selected_column}")

col_1, col_2 = st.columns([2,4])
with col_1:
    # D√©tection des outliers avec IQR
    Q1 = st.session_state["df_cleaned"][selected_column].quantile(0.25)
    Q3 = st.session_state["df_cleaned"][selected_column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = st.session_state["df_cleaned"][
        (st.session_state["df_cleaned"][selected_column] < lower_bound) |
        (st.session_state["df_cleaned"][selected_column] > upper_bound)
    ]

    # Afficher les outliers d√©tect√©s
    st.write(f"### üîé Nombre de valeurs aberrantes d√©tect√©es dans {selected_column} : {outliers.shape[0]}")
    if not outliers.empty:
        st.write(outliers[[selected_column]])

    # Option pour exclure les outliers
   # Option pour exclure les outliers avec une cl√© unique
exclude_outliers = st.checkbox("Exclure les valeurs aberrantes", key=f"exclude_{selected_column}")

with col_2:
    if exclude_outliers:
        # Supprimer les outliers du dataset stock√© dans `st.session_state`
        st.session_state["df_cleaned"] = st.session_state["df_cleaned"][
            (st.session_state["df_cleaned"][selected_column] >= lower_bound) & 
            (st.session_state["df_cleaned"][selected_column] <= upper_bound)
        ]
        
        st.write(f"### üìâ Nouvelle distribution apr√®s suppression des outliers ({st.session_state['df_cleaned'].shape[0]} valeurs restantes)")
        fig, ax = plt.subplots(figsize=(8, 5))
        sns.histplot(st.session_state["df_cleaned"][selected_column], bins=50, kde=True, ax=ax)
        plt.xlabel(selected_column)
        plt.ylabel("Fr√©quence")
        st.pyplot(fig)

        # ---------------------------------------------------------------
        # ---------------------------------------------------------------
        #  corr√©lation des variable
col_1,col_2 = st.columns([2,1])
# --- Analyse des corr√©lations ---
st.markdown("---")
with col_1:
    st.markdown("##  Analyse des Corr√©lations entre les Variables")

    # --- HEATMAP INTERACTIVE AVEC SEUIL DE CORR√âLATION ---
    st.markdown("### üî• Heatmap Interactive des Corr√©lations avec Seuil Ajustable")

    # S√©lection des colonnes num√©riques
    numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns.tolist()

    # Calcul de la matrice de corr√©lation compl√®te
    correlation_matrix = df[numeric_columns].corr()

    # Curseur interactif pour filtrer les corr√©lations
    correlation_threshold = st.slider("üîç S√©lectionnez le seuil de corr√©lation √† afficher :", 
                                    min_value=0.0, 
                                    max_value=1.0, 
                                    value=0.0,  # Par d√©faut, afficher tout
                                    step=0.05)

    # Filtrage des corr√©lations en fonction du seuil s√©lectionn√©
    filtered_corr_matrix = correlation_matrix.copy()
    mask = abs(filtered_corr_matrix) < correlation_threshold
    filtered_corr_matrix[mask] = 0  # Mettre √† z√©ro les valeurs en dessous du seuil

    # Cr√©ation de la heatmap avec Seaborn
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.heatmap(filtered_corr_matrix, annot=True, cmap="coolwarm", linewidths=0.5, fmt=".2f", center=0)
    plt.title(f"Carte de Chaleur des Corr√©lations (Seuil ‚â• {correlation_threshold})")

    # Affichage dans Streamlit
    st.pyplot(fig)


st.markdown("""
            # Pr√©parations des donn√©es d'entrainements:
            ---
            ## choix des Variables
            Nous faisons deux sets de variable pour les predictions que nous testerons:
            - **set 1:** Cet ensemble regroupe les variables qui ont une forte corr√©lation avec life_expectancy.
            
            ---

            | **Variable**                     | **Corr√©lation avec life_expectancy** |
            |----------------------------------|----------------------------------|
            | **Schooling**                    | **0.71** |
            | **Income composition of resources** | **0.69** |
            | **GDP**                           | **0.43** |
            | **Total expenditure**             | **0.46** |
            | **Adult Mortality**               | **-0.71** (corr√©lation n√©gative) |
            | **HIV/AIDS**                      | **-0.55** (corr√©lation n√©gative) |

            ---

            
            - **set 2:** Cet ensemble est con√ßu pour √©viter la colin√©arit√© et tester un mod√®le plus √©quilibr√©.
        
            ---

            | **Variable 1**                      | **Variable 2**                        | **Corr√©lation**  |
            |--------------------------------------|--------------------------------------|------------------|
            | **Total expenditure**                | **HIV/AIDS**                         | **0.37**        |
            | **Alcohol**                           | **Infant deaths**                    | **0.10**        |
            | **Schooling**                         | **Polio**                            | **0.50**        |
            | **Thinness 1-19 years**               | **BMI**                              | **-0.40**       |
            | **Income composition of resources**   | **Adult Mortality**                  | **-0.69**       |
            
            ---

            les ensemble sont divis√© en trois jeux: deux jeus d'entrainements et un jeu de verification. 

            """)

from sklearn.model_selection import train_test_split

# V√©rification que toutes les colonnes existent dans le DataFrame
columns_set1 = ["schooling", "income_composition_of_resources", "gdp", 
                "total_expenditure", "adult_mortality", "hiv/aids"]
columns_set2 = [["total_expenditure", "hiv/aids"], ["alcohol", "infant_deaths"], 
                ["schooling", "polio"], ["thinness__1-19_years", "bmi"], 
                ["income_composition_of_resources", "adult_mortality"]]

# Supprimer les valeurs manquantes pour √©viter des erreurs lors du split
df = df.dropna(subset=["life_expectancy"] + columns_set1 + [col for pair in columns_set2 for col in pair])

# D√©finition de la cible (Y) pour l'apprentissage
train_y = df["life_expectancy"]

# Jeu de donn√©es 1 : Variables fortement corr√©l√©es avec life_expectancy
train_x_set1 = df[columns_set1]

# Jeu de donn√©es 2 : Variables mod√©r√©ment corr√©l√©es et plus diversifi√©es
train_x_set2 = df[[col for pair in columns_set2 for col in pair]]

# Division en ensemble d'entra√Ænement et de test
X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(train_x_set1, train_y, test_size=0.2, random_state=42)
X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(train_x_set2, train_y, test_size=0.2, random_state=42)

# V√©rification des formes des ensembles
st.write(f"Shape du jeu de donn√©es 1 (X_train_1) : {X_train_1.shape}")
st.write(f"Shape du jeu de donn√©es 2 (X_train_2) : {X_train_2.shape}")

# -------------------------------------------------------------------------------------------------
# -------------------------------------------------------------------------------------------------
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# D√©finition des mod√®les
models = {
    "R√©gression Lin√©aire": LinearRegression(),
    "For√™t Al√©atoire": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
}

# Dictionnaires pour stocker les r√©sultats
results = {
    "Jeu de Donn√©es 1": {},
    "Jeu de Donn√©es 2": {}
}

# Entra√Ænement et √©valuation des mod√®les
for name, model in models.items():
    # Entra√Ænement sur le premier jeu de donn√©es
    model.fit(X_train_1, y_train_1)
    y_pred_1 = model.predict(X_test_1)

    # Entra√Ænement sur le deuxi√®me jeu de donn√©es
    model.fit(X_train_2, y_train_2)
    y_pred_2 = model.predict(X_test_2)

    # √âvaluation des performances pour chaque mod√®le
    results["Jeu de Donn√©es 1"][name] = {
        "MAE": mean_absolute_error(y_test_1, y_pred_1),
        "MSE": mean_squared_error(y_test_1, y_pred_1),
        "R¬≤": r2_score(y_test_1, y_pred_1)
    }

    results["Jeu de Donn√©es 2"][name] = {
        "MAE": mean_absolute_error(y_test_2, y_pred_2),
        "MSE": mean_squared_error(y_test_2, y_pred_2),
        "R¬≤": r2_score(y_test_2, y_pred_2)
    }

# Affichage des r√©sultats sous forme de tableau
import pandas as pd
df_results_1 = pd.DataFrame(results["Jeu de Donn√©es 1"]).T
df_results_2 = pd.DataFrame(results["Jeu de Donn√©es 2"]).T

st.subheader("R√©sultats des Mod√®les - Jeu de Donn√©es 1")
st.write(df_results_1)

st.subheader("R√©sultats des Mod√®les - Jeu de Donn√©es 2")
st.write(df_results_2)

st.markdown("""

        Nous avons test√© **trois mod√®les d'apprentissage supervis√©** sur nos deux jeux de donn√©es :
        1. **R√©gression Lin√©aire** 
        2. **For√™t Al√©atoire** 
        3. **Gradient Boosting** 

        ##  Crit√®res d'√âvaluation des Mod√®les

        Nous avons √©valu√© la performance des mod√®les √† l‚Äôaide de trois m√©triques principales :

        - **R¬≤ (coefficient de d√©termination) :**  
        Indique la proportion de la variance de la variable cible (Life Expectancy) expliqu√©e par le mod√®le.  
        - **Valeur proche de 1 :** le mod√®le est performant.  
        - **Valeur proche de 0 :** le mod√®le n'explique pas bien les variations de la cible.

        - **MSE (Mean Squared Error - Erreur Quadratique Moyenne) :**  
        Mesure la diff√©rence moyenne entre les valeurs pr√©dites et les valeurs r√©elles, en **√©levant au carr√©** les √©carts pour accentuer l'impact des grandes erreurs.  
        - **Plus la MSE est faible, meilleur est le mod√®le.**

        - **MAE (Mean Absolute Error - Erreur Absolue Moyenne) :**  
        Similaire √† la MSE, mais **sans √©lever au carr√©** les erreurs, ce qui la rend **moins sensible aux valeurs aberrantes**.  
        - **Plus la MAE est faible, plus la pr√©diction est pr√©cise.**

        ##  Observations

        | Mod√®le | Jeu de Donn√©es 1 (R¬≤) | Jeu de Donn√©es 2 (R¬≤) | Observation |
        |--------|----------------------|----------------------|-------------|
        | **R√©gression Lin√©aire** | 0.7707 | 0.8003 | Performances acceptables, mais limit√©es par la lin√©arit√© des relations entre les variables. |
        | **For√™t Al√©atoire** | 0.9629 | 0.9701 | Tr√®s bon mod√®le, capable de capturer des relations complexes. Faible erreur. |
        | **Gradient Boosting** | 0.9376 | 0.9531 | Excellent compromis entre complexit√© et performance, souvent plus robuste que la For√™t Al√©atoire. |

        - La **R√©gression Lin√©aire** est la plus simple, mais a du mal √† capturer les relations non lin√©aires.
        - La **For√™t Al√©atoire** et le **Gradient Boosting** offrent de bien meilleures performances, avec des valeurs R¬≤ proches de 1 et une erreur bien plus faible.
        - Le **Gradient Boosting** est g√©n√©ralement plus stable et performant pour les donn√©es complexes.

        ##  Conclusion
        Si l'on recherche un **mod√®le simple et interpr√©table**, la **R√©gression Lin√©aire** peut suffire.  
        Cependant, pour **obtenir les meilleures performances**, la **For√™t Al√©atoire** et le **Gradient Boosting** sont de meilleurs choix.  

         **Prochaine √©tape : Affiner les hyperparam√®tres des mod√®les pour optimiser encore la pr√©cision des pr√©dictions.**
        """)